# -*- coding: utf-8 -*-
"""LLM Project - Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GE-RngeqX-XnHncZgURkX8wauoZvqFkR

## **NOTE:** Please run the cells of the last section (Job Matching & Application Tool) and upload the attached pickle file in order to check functionality of the app.

## Downloading and Importing Relevant Libraries/Packages
"""

!python -m spacy download en_core_web_sm
import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip install -U sentence-transformers rank_bm25 gradio openai==0.27.7

import json
import pandas as pd
import time
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from collections import Counter
from heapq import nlargest
import nltk
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util
from openai.embeddings_utils import get_embedding, cosine_similarity

"""## Embedding the Job Postings Database"""

from google.colab import files
import pandas as pd

# Upload the PDF file
job_postings = files.upload()
df = pd.read_csv(list(job_postings.keys())[0])

df.head()

# Drop the duplicated values from each column, i.e. drop a row if it contains a duplicated value.
df=df.drop_duplicates()

# Create a column named 'combined', which containes the titles of the different reviews with the associated texts
df["combined"] = (
        "Title: " + df['title'].str.strip() +
        "; Description: " + df['description'].str.strip() +
        "; Location: " + df['location'].str.strip() +
        "; Salary: " + df['normalized_salary'].astype(str)
    )

df.head()

import re

df_combined = df.copy()

df_combined['combined'] = df_combined['combined'].apply(lambda x: re.sub('[^a-zA-Z0-9\s]','',str(x)))

# Translate all the "combined" column to lower case.
def lower_case(input_str):
    input_str = input_str.lower()
    return input_str

df_combined['combined']= df_combined['combined'].apply(lambda x: lower_case(x))

df_combined.head()

import json
from sentence_transformers import SentenceTransformer, CrossEncoder, util
import gzip
import os
import torch

"""## Use BART to summarize the 'combined' column which contains job title, description, location & salary"""

import time
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from sentence_transformers import SentenceTransformer

device = 0 if torch.cuda.is_available() else -1
summarizer = pipeline("summarization", model="facebook/bart-large-cnn", device=device)

df_combined['combined'].apply(len).mean()

"""Perform chunking where required for summarizing"""

def chunk_text(text, max_chunk_size=512):
    words = text.split()
    chunks = [' '.join(words[i:i + max_chunk_size]) for i in range(0, len(words), max_chunk_size)]
    return chunks

# Dynamic summarization function with dataset handling
def batch_summarize(texts, batch_size=8):
    summaries = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        chunked_texts = [chunk_text(text, max_chunk_size=512) for text in batch]

        # Flatten the chunks
        flat_chunks = [chunk for sublist in chunked_texts for chunk in sublist]

        # Dynamically adjust max_length and min_length for each chunk based on input length
        summarization_results = []
        for chunk in flat_chunks:
            input_length = len(chunk.split())

            if input_length < 5:  # Adjust this threshold as necessary
              print("Skipping short chunk:", chunk)
              continue

            dynamic_max_length = min(130, int(0.8 * input_length))
            dynamic_min_length = min(dynamic_max_length, max(30, int(0.5 * input_length)))  # Ensure min_length is <= max_length

            # Apply summarization with dynamically adjusted lengths
            summary = summarizer(chunk, max_length=dynamic_max_length, min_length=dynamic_min_length, do_sample=False)
            summarization_results.append(summary[0]['summary_text'])

        # Combine the summarized chunks for each original text
        batch_summaries = []
        idx = 0
        for chunk_list in chunked_texts:
            num_chunks = len(chunk_list)
            summary = ' '.join(summarization_results[idx:idx + num_chunks])
            batch_summaries.append(summary)
            idx += num_chunks

        summaries.extend(batch_summaries)

    return summaries

# Process jobs to create a 'combined' column with relevant information and summarize it
def process_jobs(df):
    # Summarize the combined column with dynamically adjusted max_length and min_length
    df['summary'] = batch_summarize(df['combined'].tolist(), batch_size=8)

    return df

df_summarized = process_jobs(df_combined)

print(df_summarized['summary'])

"""Checking if summarization worked"""

df_combined['combined'].apply(len).mean()

df_summarized['summary'].apply(len).mean()

# Save DataFrame as a CSV file
csv_filename = "df_summarized.csv"
df_summarized.to_csv(csv_filename, index=False)

# Download the file (if using Google Colab or Jupyter Notebook with download capability)
from google.colab import files
files.download(csv_filename)

df_summarized.shape

!pip install huggingface_hub

from sentence_transformers import SentenceTransformer, util
from huggingface_hub import login

# Insert your Hugging Face API token here
api_token = "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"

# Login with your token
login(api_token)

# Load the embedding model
# embedding_model = SentenceTransformer('all-mpnet-base-v2', trust_remote_code=True)
embedding_model = SentenceTransformer('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)

# Use the GPU if available
embedding_model = embedding_model.to('cuda')  # Use GPU if available

# Generate embeddings for the updated summarize column
df_summarized['embedding'] = df_summarized['summary'].apply(lambda text: embedding_model.encode(text))

import pandas as pd
from google.colab import files

pickle_filename = 'job_postings_with_embeddings.pkl'
df_summarized.to_pickle(pickle_filename)

files.download(pickle_filename)

"""## **NOTE:** **Run all cells below and upload the attached .pkl file when prompted**

# Job Matching & Application Tool
"""

!pip install huggingface_hub sentence-transformers pdfplumber gradio python-docx openai==0.28

from sentence_transformers import SentenceTransformer, util
from huggingface_hub import login
import torch

# Insert your Hugging Face API token here
api_token = "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"

# Login with your token
login(api_token)

# Import necessary libraries
import pandas as pd
import numpy as np
import gradio as gr
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import pickle

"""### Upload Pickle file containing embedded job postings"""

from google.colab import files

uploaded = files.upload()  # This will prompt the file upload dialog

# Load the pickled DataFrame from the uploaded file
with open(list(uploaded.keys())[0], 'rb') as file:
    df_embedding = pickle.load(file)

# Verify that the DataFrame is loaded
print(df_embedding.head())

"""### Define function to parse resume"""

# Function to extract text from PDF resume
def extract_text_from_pdf(file_path: str) -> str:
    """Extract text from a PDF file."""
    import pdfplumber
    try:
        with pdfplumber.open(file_path) as pdf:
            full_text = [page.extract_text() for page in pdf.pages]
        return '\n'.join(full_text).strip()
    except Exception as e:
        logging.error(f"Error extracting text from PDF file: {e}")
        raise

"""### Load embedding model for embedding resume"""

# Load the embedding model
embedding_model = SentenceTransformer('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)

"""### Search Engine using Cosine Similarity to match Resume to Job Postings"""

# Function to perform search based on cosine similarity
def search_job_postings(resume, df=df_embedding, top_n=3):
       # Extract embeddings from the DataFrame
    embeddings = np.vstack(df['embedding'].values)

    # Compute cosine similarity between the resume embedding and all job posting embeddings
    cosine_similarities = cosine_similarity([resume], embeddings)[0]

    # Get the indices of the top N most similar jobs
    top_indices = np.argsort(cosine_similarities)[-top_n:][::-1]  # Sort in descending order

    # Prepare the results
    results = []
    for index in top_indices:
        company_name = df.loc[index, 'company_name']
        job_title = df.loc[index, 'title']
        job_decription = df.loc[index, 'description']
        location = df.loc[index, 'location']
        salary = df.loc[index, 'normalized_salary']
        skills = df.loc[index, 'skills_desc']
        similarity_score = cosine_similarities[index]

        results.append({
            'Company': company_name,
            'Job Title': job_title,
            'Job Description': job_decription,
            'Location': location,
            'Salary': salary,
            'Skills': skills,
            'Application URL': df.loc[index, 'job_posting_url'],
            'Similarity Score': similarity_score
        })
    return results

"""### Define Agents and their functions"""

class Agent:
    """A class representing an agent to perform a specific task."""
    def __init__(self, role, goal, action_function):
        self.role = role
        self.goal = goal
        self.action_function = action_function

    def perform_action(self, *args, **kwargs):
        """Execute the agent's task."""
        return self.action_function(*args, **kwargs)

search_agent = Agent(
    role='Search Agent',
    goal='Find the top 3 job matches based on the uploaded resume.',
    action_function=lambda resume: search_job_postings(resume = resume)
)

cover_letter_agent = Agent(
    role='Cover Letter Agent',
    goal='Generate a customized cover letter for the top job match.',
    action_function=lambda resume, job: generate_cover_letter(resume, job)
)

cold_email_agent = Agent(
    role='Cold Email Agent',
    goal='Generate a cold email to the recruiter for the top job match.',
    action_function=lambda resume, job: generate_cold_email(resume, job)
)

import openai
import docx
from docx.shared import Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from google.colab import userdata
import os
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email.mime.text import MIMEText
from email import encoders

openai.api_key = userdata.get('OPENAI_API_KEY')
os.environ["OPENAI_API_KEY"] = openai.api_key

def generate_cover_letter(resume_text, job):
    """Generate a cover letter using GPT."""
    prompt = f"""
    Write a professional cover letter for the following job:
    Job Title: {job['Job Title']}
    Company: {job['Company']}
    Location: {job['Location']}
    Description: {job['Job Description']}

    Use this resume for context on the applicant's name, background, contact information and address: {resume_text}
    """
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()

def save_cover_letter_as_docx(cover_letter, filename="Cover Letter.docx"):
    """Save the cover letter as a DOCX file with formatting."""
    doc = docx.Document()

    # Add a bold heading
    heading = doc.add_heading('COVER LETTER', level=1)
    heading.bold = True
    heading.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER  # Center the heading

    # Add the cover letter text
    paragraph = doc.add_paragraph(cover_letter)
    paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT  # Justify the text

    # Optional: Set font size (e.g., 12 pt)
    for run in paragraph.runs:
        run.font.size = Pt(12)

    doc.save(filename)
    return filename

def generate_cold_email(resume_text, job):
    """Generate a cold email using GPT."""
    prompt = f"""
    Write a polite cold email to the recruiter for this job:
    Job Title: {job['Job Title']}
    Company: {job['Company']}

    Use this resume for context on the applicant's name, background, contact information and address: {resume_text}
    """
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()

def save_email_as_eml(subject, body, to_address, attachment=None):
    msg = MIMEMultipart()
    msg['Subject'] = subject
    msg['To'] = to_address
    msg.attach(MIMEText(body, 'plain'))

    if attachment:
        # Create a MIMEBase object for the attachment
        part = MIMEBase('application', 'octet-stream')
        with open(attachment, 'rb') as f:
            part.set_payload(f.read())
        encoders.encode_base64(part)
        part.add_header('Content-Disposition', f'attachment; filename={os.path.basename(attachment)}')
        msg.attach(part)

    # Save the email as .eml
    filename = f"Recruiter Email.eml"
    with open(filename, 'w') as f:
        f.write(msg.as_string())

    return filename

"""### Launch App"""

import os

def interface_function(resume_file):
    try:
        resume_text = extract_text_from_pdf(resume_file.name)
    except Exception as e:
        return "Failed to extract text from the uploaded resume: " + str(e), None, None

    # Get embedded representation of the resume
    resume_embedded = embedding_model.encode(resume_text)
    top_jobs = search_agent.perform_action(resume_embedded)

    # Prepare results
    results = {}

    # Show Top 3 Job Matches with detailed information
    job_details = []
    for i, job in enumerate(top_jobs[:3]):
        job_info = (f"{i+1}. Job Title: {job['Job Title']}\n"
                    f"   Company: {job['Company']}\n"
                    f"   Location: {job['Location']}\n"
                    f"   Salary: {job['Salary']}\n"
                    f"   Application URL: {job['Application URL']}\n")
        job_details.append(job_info)

    top_job_matches = "\n\n".join(job_details)
    results['Top Job Matches'] = top_job_matches

    cover_letter_filename = None
    cold_email_filename = None

    # Only generate cover letter and email for the top job
    top_job = top_jobs[0]
    cover_letter = cover_letter_agent.perform_action(resume_text, top_job)
    cover_letter_filename = save_cover_letter_as_docx(cover_letter)

    cold_email = cold_email_agent.perform_action(resume_text, top_job)
    email_subject = f"Application for {top_job['Job Title']} at {top_job['Company']}"
    cold_email_filename = save_email_as_eml(email_subject, cold_email, "" , resume_file)

    return results['Top Job Matches'], cover_letter_filename, cold_email_filename

# Gradio Interface
interface = gr.Interface(
    fn=interface_function,
    inputs=[
        gr.File(label="Upload Your Resume (PDF)"),
    ],
    outputs=[
        gr.Textbox(label="Job Matches", lines=10),
        gr.File(label="Download Cover Letter (DOCX)"),
        gr.File(label="Download Cold Email (EML)"),
    ],
    title="AI Job Application Assistant",
    description="Upload your resume to find job matches, generate cover letters, and write cold emails."
)

# Launch the Gradio App
interface.launch(debug=True)

